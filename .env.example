# ===== LLM Configuration =====
# vLLM server endpoint (OpenAI-compatible API)
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_API_KEY=EMPTY
VLLM_MODEL_NAME=lapa-llm/lapa-v0.1.2-instruct
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2048


# Use hosted embeddings API (faster setup, requires API key)
# Set to false to use local sentence-transformers
USE_HOSTED_EMBEDDINGS=true



# ===== Agent Configuration =====
MAX_CONVERSATION_HISTORY=20

# ===== Optional: OpenAI Fallback =====
# Uncomment if you want to use OpenAI API instead of vLLM for testing
# OPENAI_API_KEY=your_openai_api_key_here
# USE_OPENAI_FALLBACK=true

# ===== Debug Settings =====
DEBUG_MODE=false
LOG_LEVEL=INFO

# ===== LangSmith Tracing (optional) =====
# Get API key from https://smith.langchain.com/
LANGCHAIN_TRACING_V2=false
LANGCHAIN_API_KEY=your_langsmith_api_key_here
LANGCHAIN_PROJECT=graphiti-lapa-demo

# ===== Qdrant Config =====
QDRANT_URL=http://qdrant:6333
QDRANT_API_KEY= 
QDRANT_COLLECTION=facts
QDRANT_DISTANCE=cosine