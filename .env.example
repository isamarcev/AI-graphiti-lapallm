# ===== LLM Configuration =====
# vLLM server endpoint (OpenAI-compatible API)
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_API_KEY=EMPTY
VLLM_MODEL_NAME=lapa-llm/lapa-v0.1.2-instruct
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2048

# ===== Neo4j Configuration =====
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password123
NEO4J_DATABASE=neo4j

# ===== Graphiti Configuration =====
GRAPHITI_MAX_EPISODE_LENGTH=10000
GRAPHITI_SEARCH_LIMIT=10
GRAPHITI_RELEVANCE_THRESHOLD=0.7

# ===== Embeddings Configuration =====
# Multilingual model with Ukrainian support
EMBEDDING_MODEL_NAME=sentence-transformers/paraphrase-multilingual-mpnet-base-v2
EMBEDDING_DIMENSION=768

# Use hosted embeddings API (faster setup, requires API key)
# Set to false to use local sentence-transformers
USE_HOSTED_EMBEDDINGS=true

# ===== Reranking Configuration =====
# RERANKER_TYPE options:
#   - "noop": No reranking (fastest, 3-10x faster than BGE)
#   - "bge": Local BGE cross-encoder (most accurate, VERY slow on CPU)
#   - "hosted": Use Lapathon API for reranking (medium speed, requires API)
RERANKER_TYPE=noop

# Legacy flag (for backward compatibility)
# If true and RERANKER_TYPE=noop, will use BGE instead
USE_RERANKER=false

# Hosted reranker settings (only for RERANKER_TYPE=hosted)
RERANKER_USE_LOGPROBS=true
RERANKER_MAX_CONCURRENT=10  # Higher = faster (10-20 recommended, watch for rate limits)

# ===== Agent Configuration =====
MAX_CONVERSATION_HISTORY=20

# ===== Optional: OpenAI Fallback =====
# Uncomment if you want to use OpenAI API instead of vLLM for testing
# OPENAI_API_KEY=your_openai_api_key_here
# USE_OPENAI_FALLBACK=true

# ===== Debug Settings =====
DEBUG_MODE=false
LOG_LEVEL=INFO

# ===== LangSmith Tracing (optional) =====
# Get API key from https://smith.langchain.com/
LANGCHAIN_TRACING_V2=false
LANGCHAIN_API_KEY=your_langsmith_api_key_here
LANGCHAIN_PROJECT=graphiti-lapa-demo
