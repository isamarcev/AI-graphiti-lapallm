{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReAct logic test notebook\n",
    "\n",
    "Цей ноутбук допомагає швидко перевірити підтримку structured output у LLM\n",
    "та прогнати `react_loop_node` на простому прикладі.\n",
    "\n",
    "Перед запуском переконайтеся, що vLLM/OpenAI endpoint і Graphiti доступні\n",
    "та налаштовані через `config.settings` або змінні оточення."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T19:50:49.263619Z",
     "start_time": "2026-01-15T19:50:49.172395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import asyncio\n",
    "\n",
    "repo_root = Path(\"../graphity_lapa\")\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from clients.llm_client import get_llm_client\n",
    "from agent.nodes.react import react_loop_node, ReactStep\n",
    "from agent.state import create_initial_state"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T17:39:19.863332Z",
     "start_time": "2026-01-15T17:39:14.278965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Перевірка structured output\n",
    "llm = get_llm_client()\n",
    "\n",
    "prompt = \"\"\"Поверни JSON з ключами: thought, action, query.\n",
    "thought: 'test', action: 'answer', query: ''\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    result = await llm.generate_async(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format=ReactStep,\n",
    "        temperature=0.0,\n",
    "        max_tokens=50,\n",
    "        timeout=2\n",
    "    )\n",
    "    print(result)\n",
    "    print(type(result))\n",
    "except Exception as exc:\n",
    "    print(\"Structured output не підтримується або виникла помилка:\", exc)"
   ],
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mCancelledError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      4\u001B[39m prompt = \u001B[33m\"\"\"\u001B[39m\u001B[33mПоверни JSON з ключами: thought, action, query.\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[33mthought: \u001B[39m\u001B[33m'\u001B[39m\u001B[33mtest\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, action: \u001B[39m\u001B[33m'\u001B[39m\u001B[33manswer\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, query: \u001B[39m\u001B[33m'\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[33m\"\"\"\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m     result = \u001B[38;5;28;01mawait\u001B[39;00m llm.generate_async(\n\u001B[32m     10\u001B[39m         messages=[{\u001B[33m\"\u001B[39m\u001B[33mrole\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m: prompt}],\n\u001B[32m     11\u001B[39m         response_format=ReactStep,\n\u001B[32m     12\u001B[39m         temperature=\u001B[32m0.0\u001B[39m,\n\u001B[32m     13\u001B[39m         max_tokens=\u001B[32m50\u001B[39m,\n\u001B[32m     14\u001B[39m         timeout=\u001B[32m2\u001B[39m\n\u001B[32m     15\u001B[39m     )\n\u001B[32m     16\u001B[39m     \u001B[38;5;28mprint\u001B[39m(result)\n\u001B[32m     17\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mtype\u001B[39m(result))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/clients/llm_client.py:108\u001B[39m, in \u001B[36mLLMClient.generate_async\u001B[39m\u001B[34m(self, messages, temperature, max_tokens, response_format, **kwargs)\u001B[39m\n\u001B[32m    105\u001B[39m     llm = llm.with_structured_output(response_format)\n\u001B[32m    107\u001B[39m \u001B[38;5;66;03m# Invoke LLM\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m response = \u001B[38;5;28;01mawait\u001B[39;00m llm.ainvoke(lc_messages)\n\u001B[32m    110\u001B[39m \u001B[38;5;66;03m# If structured output, return directly\u001B[39;00m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m response_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3191\u001B[39m, in \u001B[36mRunnableSequence.ainvoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   3189\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3190\u001B[39m                 part = functools.partial(step.ainvoke, input_, config)\n\u001B[32m-> \u001B[39m\u001B[32m3191\u001B[39m             input_ = \u001B[38;5;28;01mawait\u001B[39;00m coro_with_context(part(), context, create_task=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m   3192\u001B[39m     \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[32m   3193\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5570\u001B[39m, in \u001B[36mRunnableBindingBase.ainvoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   5563\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   5564\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mainvoke\u001B[39m(\n\u001B[32m   5565\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   5568\u001B[39m     **kwargs: Any | \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   5569\u001B[39m ) -> Output:\n\u001B[32m-> \u001B[39m\u001B[32m5570\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.bound.ainvoke(\n\u001B[32m   5571\u001B[39m         \u001B[38;5;28minput\u001B[39m,\n\u001B[32m   5572\u001B[39m         \u001B[38;5;28mself\u001B[39m._merge_configs(config),\n\u001B[32m   5573\u001B[39m         **{**\u001B[38;5;28mself\u001B[39m.kwargs, **kwargs},\n\u001B[32m   5574\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:425\u001B[39m, in \u001B[36mBaseChatModel.ainvoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    415\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    416\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mainvoke\u001B[39m(\n\u001B[32m    417\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    422\u001B[39m     **kwargs: Any,\n\u001B[32m    423\u001B[39m ) -> AIMessage:\n\u001B[32m    424\u001B[39m     config = ensure_config(config)\n\u001B[32m--> \u001B[39m\u001B[32m425\u001B[39m     llm_result = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.agenerate_prompt(\n\u001B[32m    426\u001B[39m         [\u001B[38;5;28mself\u001B[39m._convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[32m    427\u001B[39m         stop=stop,\n\u001B[32m    428\u001B[39m         callbacks=config.get(\u001B[33m\"\u001B[39m\u001B[33mcallbacks\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    429\u001B[39m         tags=config.get(\u001B[33m\"\u001B[39m\u001B[33mtags\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    430\u001B[39m         metadata=config.get(\u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    431\u001B[39m         run_name=config.get(\u001B[33m\"\u001B[39m\u001B[33mrun_name\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    432\u001B[39m         run_id=config.pop(\u001B[33m\"\u001B[39m\u001B[33mrun_id\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m    433\u001B[39m         **kwargs,\n\u001B[32m    434\u001B[39m     )\n\u001B[32m    435\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[32m    436\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAIMessage\u001B[39m\u001B[33m\"\u001B[39m, cast(\u001B[33m\"\u001B[39m\u001B[33mChatGeneration\u001B[39m\u001B[33m\"\u001B[39m, llm_result.generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m]).message\n\u001B[32m    437\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1132\u001B[39m, in \u001B[36mBaseChatModel.agenerate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m   1123\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   1124\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34magenerate_prompt\u001B[39m(\n\u001B[32m   1125\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1129\u001B[39m     **kwargs: Any,\n\u001B[32m   1130\u001B[39m ) -> LLMResult:\n\u001B[32m   1131\u001B[39m     prompt_messages = [p.to_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m-> \u001B[39m\u001B[32m1132\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.agenerate(\n\u001B[32m   1133\u001B[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001B[32m   1134\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1052\u001B[39m, in \u001B[36mBaseChatModel.agenerate\u001B[39m\u001B[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m   1039\u001B[39m run_managers = \u001B[38;5;28;01mawait\u001B[39;00m callback_manager.on_chat_model_start(\n\u001B[32m   1040\u001B[39m     \u001B[38;5;28mself\u001B[39m._serialized,\n\u001B[32m   1041\u001B[39m     messages_to_trace,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1046\u001B[39m     run_id=run_id,\n\u001B[32m   1047\u001B[39m )\n\u001B[32m   1049\u001B[39m input_messages = [\n\u001B[32m   1050\u001B[39m     _normalize_messages(message_list) \u001B[38;5;28;01mfor\u001B[39;00m message_list \u001B[38;5;129;01min\u001B[39;00m messages\n\u001B[32m   1051\u001B[39m ]\n\u001B[32m-> \u001B[39m\u001B[32m1052\u001B[39m results = \u001B[38;5;28;01mawait\u001B[39;00m asyncio.gather(\n\u001B[32m   1053\u001B[39m     *[\n\u001B[32m   1054\u001B[39m         \u001B[38;5;28mself\u001B[39m._agenerate_with_cache(\n\u001B[32m   1055\u001B[39m             m,\n\u001B[32m   1056\u001B[39m             stop=stop,\n\u001B[32m   1057\u001B[39m             run_manager=run_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1058\u001B[39m             **kwargs,\n\u001B[32m   1059\u001B[39m         )\n\u001B[32m   1060\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages)\n\u001B[32m   1061\u001B[39m     ],\n\u001B[32m   1062\u001B[39m     return_exceptions=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m   1063\u001B[39m )\n\u001B[32m   1064\u001B[39m exceptions = []\n\u001B[32m   1065\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, res \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(results):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1359\u001B[39m, in \u001B[36mBaseChatModel._agenerate_with_cache\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1357\u001B[39m     result = generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[32m   1358\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m inspect.signature(\u001B[38;5;28mself\u001B[39m._agenerate).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1359\u001B[39m     result = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._agenerate(\n\u001B[32m   1360\u001B[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001B[32m   1361\u001B[39m     )\n\u001B[32m   1362\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1363\u001B[39m     result = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1597\u001B[39m, in \u001B[36mBaseChatOpenAI._agenerate\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1595\u001B[39m payload.pop(\u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1596\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1597\u001B[39m     raw_response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.root_async_client.chat.completions.with_raw_response.parse(  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[32m   1598\u001B[39m         **payload\n\u001B[32m   1599\u001B[39m     )\n\u001B[32m   1600\u001B[39m     response = raw_response.parse()\n\u001B[32m   1601\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m openai.BadRequestError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/openai/_legacy_response.py:381\u001B[39m, in \u001B[36masync_to_raw_response_wrapper.<locals>.wrapped\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    377\u001B[39m extra_headers[RAW_RESPONSE_HEADER] = \u001B[33m\"\u001B[39m\u001B[33mtrue\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    379\u001B[39m kwargs[\u001B[33m\"\u001B[39m\u001B[33mextra_headers\u001B[39m\u001B[33m\"\u001B[39m] = extra_headers\n\u001B[32m--> \u001B[39m\u001B[32m381\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m cast(LegacyAPIResponse[R], \u001B[38;5;28;01mawait\u001B[39;00m func(*args, **kwargs))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1670\u001B[39m, in \u001B[36mAsyncCompletions.parse\u001B[39m\u001B[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m   1663\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mparser\u001B[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001B[32m   1664\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _parse_chat_completion(\n\u001B[32m   1665\u001B[39m         response_format=response_format,\n\u001B[32m   1666\u001B[39m         chat_completion=raw_completion,\n\u001B[32m   1667\u001B[39m         input_tools=tools,\n\u001B[32m   1668\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1670\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._post(\n\u001B[32m   1671\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m/chat/completions\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1672\u001B[39m     body=\u001B[38;5;28;01mawait\u001B[39;00m async_maybe_transform(\n\u001B[32m   1673\u001B[39m         {\n\u001B[32m   1674\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m: messages,\n\u001B[32m   1675\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: model,\n\u001B[32m   1676\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33maudio\u001B[39m\u001B[33m\"\u001B[39m: audio,\n\u001B[32m   1677\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mfrequency_penalty\u001B[39m\u001B[33m\"\u001B[39m: frequency_penalty,\n\u001B[32m   1678\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mfunction_call\u001B[39m\u001B[33m\"\u001B[39m: function_call,\n\u001B[32m   1679\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mfunctions\u001B[39m\u001B[33m\"\u001B[39m: functions,\n\u001B[32m   1680\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mlogit_bias\u001B[39m\u001B[33m\"\u001B[39m: logit_bias,\n\u001B[32m   1681\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mlogprobs\u001B[39m\u001B[33m\"\u001B[39m: logprobs,\n\u001B[32m   1682\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mmax_completion_tokens\u001B[39m\u001B[33m\"\u001B[39m: max_completion_tokens,\n\u001B[32m   1683\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mmax_tokens\u001B[39m\u001B[33m\"\u001B[39m: max_tokens,\n\u001B[32m   1684\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: metadata,\n\u001B[32m   1685\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mmodalities\u001B[39m\u001B[33m\"\u001B[39m: modalities,\n\u001B[32m   1686\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mn\u001B[39m\u001B[33m\"\u001B[39m: n,\n\u001B[32m   1687\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mparallel_tool_calls\u001B[39m\u001B[33m\"\u001B[39m: parallel_tool_calls,\n\u001B[32m   1688\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mprediction\u001B[39m\u001B[33m\"\u001B[39m: prediction,\n\u001B[32m   1689\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mpresence_penalty\u001B[39m\u001B[33m\"\u001B[39m: presence_penalty,\n\u001B[32m   1690\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mprompt_cache_key\u001B[39m\u001B[33m\"\u001B[39m: prompt_cache_key,\n\u001B[32m   1691\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mprompt_cache_retention\u001B[39m\u001B[33m\"\u001B[39m: prompt_cache_retention,\n\u001B[32m   1692\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mreasoning_effort\u001B[39m\u001B[33m\"\u001B[39m: reasoning_effort,\n\u001B[32m   1693\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mresponse_format\u001B[39m\u001B[33m\"\u001B[39m: _type_to_response_format(response_format),\n\u001B[32m   1694\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33msafety_identifier\u001B[39m\u001B[33m\"\u001B[39m: safety_identifier,\n\u001B[32m   1695\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mseed\u001B[39m\u001B[33m\"\u001B[39m: seed,\n\u001B[32m   1696\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mservice_tier\u001B[39m\u001B[33m\"\u001B[39m: service_tier,\n\u001B[32m   1697\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mstore\u001B[39m\u001B[33m\"\u001B[39m: store,\n\u001B[32m   1698\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mstop\u001B[39m\u001B[33m\"\u001B[39m: stop,\n\u001B[32m   1699\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   1700\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mstream_options\u001B[39m\u001B[33m\"\u001B[39m: stream_options,\n\u001B[32m   1701\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mtemperature\u001B[39m\u001B[33m\"\u001B[39m: temperature,\n\u001B[32m   1702\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mtool_choice\u001B[39m\u001B[33m\"\u001B[39m: tool_choice,\n\u001B[32m   1703\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mtools\u001B[39m\u001B[33m\"\u001B[39m: tools,\n\u001B[32m   1704\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mtop_logprobs\u001B[39m\u001B[33m\"\u001B[39m: top_logprobs,\n\u001B[32m   1705\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mtop_p\u001B[39m\u001B[33m\"\u001B[39m: top_p,\n\u001B[32m   1706\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m: user,\n\u001B[32m   1707\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mverbosity\u001B[39m\u001B[33m\"\u001B[39m: verbosity,\n\u001B[32m   1708\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mweb_search_options\u001B[39m\u001B[33m\"\u001B[39m: web_search_options,\n\u001B[32m   1709\u001B[39m         },\n\u001B[32m   1710\u001B[39m         completion_create_params.CompletionCreateParams,\n\u001B[32m   1711\u001B[39m     ),\n\u001B[32m   1712\u001B[39m     options=make_request_options(\n\u001B[32m   1713\u001B[39m         extra_headers=extra_headers,\n\u001B[32m   1714\u001B[39m         extra_query=extra_query,\n\u001B[32m   1715\u001B[39m         extra_body=extra_body,\n\u001B[32m   1716\u001B[39m         timeout=timeout,\n\u001B[32m   1717\u001B[39m         post_parser=parser,\n\u001B[32m   1718\u001B[39m     ),\n\u001B[32m   1719\u001B[39m     \u001B[38;5;66;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001B[39;00m\n\u001B[32m   1720\u001B[39m     \u001B[38;5;66;03m# in the `parser` function above\u001B[39;00m\n\u001B[32m   1721\u001B[39m     cast_to=cast(Type[ParsedChatCompletion[ResponseFormatT]], ChatCompletion),\n\u001B[32m   1722\u001B[39m     stream=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   1723\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/openai/_base_client.py:1797\u001B[39m, in \u001B[36mAsyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1784\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1785\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1792\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_AsyncStreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1793\u001B[39m ) -> ResponseT | _AsyncStreamT:\n\u001B[32m   1794\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1795\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=\u001B[38;5;28;01mawait\u001B[39;00m async_to_httpx_files(files), **options\n\u001B[32m   1796\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1797\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/openai/_base_client.py:1532\u001B[39m, in \u001B[36mAsyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1530\u001B[39m response = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1531\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1532\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._client.send(\n\u001B[32m   1533\u001B[39m         request,\n\u001B[32m   1534\u001B[39m         stream=stream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._should_stream_response_body(request=request),\n\u001B[32m   1535\u001B[39m         **kwargs,\n\u001B[32m   1536\u001B[39m     )\n\u001B[32m   1537\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m httpx.TimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m   1538\u001B[39m     log.debug(\u001B[33m\"\u001B[39m\u001B[33mEncountered httpx.TimeoutException\u001B[39m\u001B[33m\"\u001B[39m, exc_info=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpx/_client.py:1629\u001B[39m, in \u001B[36mAsyncClient.send\u001B[39m\u001B[34m(self, request, stream, auth, follow_redirects)\u001B[39m\n\u001B[32m   1625\u001B[39m \u001B[38;5;28mself\u001B[39m._set_timeout(request)\n\u001B[32m   1627\u001B[39m auth = \u001B[38;5;28mself\u001B[39m._build_request_auth(request, auth)\n\u001B[32m-> \u001B[39m\u001B[32m1629\u001B[39m response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._send_handling_auth(\n\u001B[32m   1630\u001B[39m     request,\n\u001B[32m   1631\u001B[39m     auth=auth,\n\u001B[32m   1632\u001B[39m     follow_redirects=follow_redirects,\n\u001B[32m   1633\u001B[39m     history=[],\n\u001B[32m   1634\u001B[39m )\n\u001B[32m   1635\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1636\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpx/_client.py:1657\u001B[39m, in \u001B[36mAsyncClient._send_handling_auth\u001B[39m\u001B[34m(self, request, auth, follow_redirects, history)\u001B[39m\n\u001B[32m   1654\u001B[39m request = \u001B[38;5;28;01mawait\u001B[39;00m auth_flow.\u001B[34m__anext__\u001B[39m()\n\u001B[32m   1656\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1657\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._send_handling_redirects(\n\u001B[32m   1658\u001B[39m         request,\n\u001B[32m   1659\u001B[39m         follow_redirects=follow_redirects,\n\u001B[32m   1660\u001B[39m         history=history,\n\u001B[32m   1661\u001B[39m     )\n\u001B[32m   1662\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1663\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpx/_client.py:1694\u001B[39m, in \u001B[36mAsyncClient._send_handling_redirects\u001B[39m\u001B[34m(self, request, follow_redirects, history)\u001B[39m\n\u001B[32m   1691\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mrequest\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1692\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m hook(request)\n\u001B[32m-> \u001B[39m\u001B[32m1694\u001B[39m response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._send_single_request(request)\n\u001B[32m   1695\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1696\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mresponse\u001B[39m\u001B[33m\"\u001B[39m]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpx/_client.py:1730\u001B[39m, in \u001B[36mAsyncClient._send_single_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m   1725\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   1726\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAttempted to send an sync request with an AsyncClient instance.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1727\u001B[39m     )\n\u001B[32m   1729\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=request):\n\u001B[32m-> \u001B[39m\u001B[32m1730\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m transport.handle_async_request(request)\n\u001B[32m   1732\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, AsyncByteStream)\n\u001B[32m   1733\u001B[39m response.request = request\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394\u001B[39m, in \u001B[36mAsyncHTTPTransport.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    381\u001B[39m req = httpcore.Request(\n\u001B[32m    382\u001B[39m     method=request.method,\n\u001B[32m    383\u001B[39m     url=httpcore.URL(\n\u001B[32m   (...)\u001B[39m\u001B[32m    391\u001B[39m     extensions=request.extensions,\n\u001B[32m    392\u001B[39m )\n\u001B[32m    393\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m--> \u001B[39m\u001B[32m394\u001B[39m     resp = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pool.handle_async_request(req)\n\u001B[32m    396\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp.stream, typing.AsyncIterable)\n\u001B[32m    398\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m Response(\n\u001B[32m    399\u001B[39m     status_code=resp.status,\n\u001B[32m    400\u001B[39m     headers=resp.headers,\n\u001B[32m    401\u001B[39m     stream=AsyncResponseStream(resp.stream),\n\u001B[32m    402\u001B[39m     extensions=resp.extensions,\n\u001B[32m    403\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256\u001B[39m, in \u001B[36mAsyncConnectionPool.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    253\u001B[39m         closing = \u001B[38;5;28mself\u001B[39m._assign_requests_to_connections()\n\u001B[32m    255\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._close_connections(closing)\n\u001B[32m--> \u001B[39m\u001B[32m256\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    258\u001B[39m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[32m    259\u001B[39m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n\u001B[32m    260\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, typing.AsyncIterable)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236\u001B[39m, in \u001B[36mAsyncConnectionPool.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    232\u001B[39m connection = \u001B[38;5;28;01mawait\u001B[39;00m pool_request.wait_for_connection(timeout=timeout)\n\u001B[32m    234\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    235\u001B[39m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m connection.handle_async_request(\n\u001B[32m    237\u001B[39m         pool_request.request\n\u001B[32m    238\u001B[39m     )\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[32m    240\u001B[39m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[32m    241\u001B[39m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[32m    242\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    243\u001B[39m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n\u001B[32m    244\u001B[39m     pool_request.clear_connection()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103\u001B[39m, in \u001B[36mAsyncHTTPConnection.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    100\u001B[39m     \u001B[38;5;28mself\u001B[39m._connect_failed = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    101\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connection.handle_async_request(request)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:136\u001B[39m, in \u001B[36mAsyncHTTP11Connection.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    134\u001B[39m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[33m\"\u001B[39m\u001B[33mresponse_closed\u001B[39m\u001B[33m\"\u001B[39m, logger, request) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m    135\u001B[39m         \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._response_closed()\n\u001B[32m--> \u001B[39m\u001B[32m136\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:106\u001B[39m, in \u001B[36mAsyncHTTP11Connection.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m     95\u001B[39m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m     97\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\n\u001B[32m     98\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mreceive_response_headers\u001B[39m\u001B[33m\"\u001B[39m, logger, request, kwargs\n\u001B[32m     99\u001B[39m ) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m    100\u001B[39m     (\n\u001B[32m    101\u001B[39m         http_version,\n\u001B[32m    102\u001B[39m         status,\n\u001B[32m    103\u001B[39m         reason_phrase,\n\u001B[32m    104\u001B[39m         headers,\n\u001B[32m    105\u001B[39m         trailing_data,\n\u001B[32m--> \u001B[39m\u001B[32m106\u001B[39m     ) = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._receive_response_headers(**kwargs)\n\u001B[32m    107\u001B[39m     trace.return_value = (\n\u001B[32m    108\u001B[39m         http_version,\n\u001B[32m    109\u001B[39m         status,\n\u001B[32m    110\u001B[39m         reason_phrase,\n\u001B[32m    111\u001B[39m         headers,\n\u001B[32m    112\u001B[39m     )\n\u001B[32m    114\u001B[39m network_stream = \u001B[38;5;28mself\u001B[39m._network_stream\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:177\u001B[39m, in \u001B[36mAsyncHTTP11Connection._receive_response_headers\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    174\u001B[39m timeout = timeouts.get(\u001B[33m\"\u001B[39m\u001B[33mread\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m     event = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._receive_event(timeout=timeout)\n\u001B[32m    178\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11.Response):\n\u001B[32m    179\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:217\u001B[39m, in \u001B[36mAsyncHTTP11Connection._receive_event\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    214\u001B[39m     event = \u001B[38;5;28mself\u001B[39m._h11_state.next_event()\n\u001B[32m    216\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11.NEED_DATA:\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m     data = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._network_stream.read(\n\u001B[32m    218\u001B[39m         \u001B[38;5;28mself\u001B[39m.READ_NUM_BYTES, timeout=timeout\n\u001B[32m    219\u001B[39m     )\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    227\u001B[39m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[32m    228\u001B[39m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[32m    229\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m data == \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:35\u001B[39m, in \u001B[36mAnyIOStream.read\u001B[39m\u001B[34m(self, max_bytes, timeout)\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m anyio.fail_after(timeout):\n\u001B[32m     34\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._stream.receive(max_bytes=max_bytes)\n\u001B[32m     36\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m anyio.EndOfStream:  \u001B[38;5;66;03m# pragma: nocover\u001B[39;00m\n\u001B[32m     37\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1269\u001B[39m, in \u001B[36mSocketStream.receive\u001B[39m\u001B[34m(self, max_bytes)\u001B[39m\n\u001B[32m   1263\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   1264\u001B[39m     \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._protocol.read_event.is_set()\n\u001B[32m   1265\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._transport.is_closing()\n\u001B[32m   1266\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._protocol.is_at_eof\n\u001B[32m   1267\u001B[39m ):\n\u001B[32m   1268\u001B[39m     \u001B[38;5;28mself\u001B[39m._transport.resume_reading()\n\u001B[32m-> \u001B[39m\u001B[32m1269\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._protocol.read_event.wait()\n\u001B[32m   1270\u001B[39m     \u001B[38;5;28mself\u001B[39m._transport.pause_reading()\n\u001B[32m   1271\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/locks.py:212\u001B[39m, in \u001B[36mEvent.wait\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    210\u001B[39m \u001B[38;5;28mself\u001B[39m._waiters.append(fut)\n\u001B[32m    211\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m212\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m fut\n\u001B[32m    213\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    214\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[31mCancelledError\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T15:55:45.204585Z",
     "start_time": "2026-01-15T15:55:44.329214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Прогін react_loop_node\n",
    "state = create_initial_state(\n",
    "    message_uid=\"demo-1\",\n",
    "    message_text=\"Поясни, що таке ReAct агент простими словами\",\n",
    "    user_id=\"local_test\",\n",
    ")\n",
    "\n",
    "# Приклад додаткового контексту (якщо потрібно)\n",
    "state[\"retrieved_context\"] = [\n",
    "    {\n",
    "        \"source_msg_uid\": \"seed-1\",\n",
    "        \"content\": \"ReAct поєднує reasoning і acting, чергуючи думки та дії.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "result = await react_loop_node(state)\n",
    "print(result)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/creator/Documents/Creator/PythonProject/graphity_lapa/clients/qdrant_client.py:51: UserWarning: Api key is used with an insecure connection.\n",
      "  self._client = AsyncQdrantClient(url=self.url, api_key=self.api_key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'react_steps': [{'thought': 'ReAct – це агент, який поєднує міркування та дії, чергуючи думки та дії.', 'action': 'answer', 'observation': 'Готово до генерації відповіді'}]}\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
