{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphiti + LangGraph + Lapa LLM Demo\n",
    "\n",
    "This notebook demonstrates an AI agent with long-term memory using:\n",
    "- **Lapa LLM** - Ukrainian language model via hosted Lapathon API\n",
    "- **Graphiti** - Temporal knowledge graph for memory\n",
    "- **LangGraph** - Agent orchestration\n",
    "- **Neo4j** - Graph database storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Test Graphiti Message Alternation Fix\n",
    "\n",
    "Test that our wrapper correctly handles message role alternation issues"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T20:57:26.703472Z",
     "start_time": "2026-01-17T20:57:26.700211Z"
    }
   },
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, '../')\n",
    "# Enable detailed logging"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T20:57:29.817429Z",
     "start_time": "2026-01-17T20:57:28.491433Z"
    }
   },
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import our modules\n",
    "from config.settings import settings\n",
    "from clients.llm_client import get_llm_client\n",
    "from agent.graph import get_agent_app\n",
    "from agent.state import create_initial_state\n",
    "from utils.langsmith_setup import setup_langsmith\n",
    "\n",
    "# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è LangSmith\n",
    "setup_langsmith()\n",
    "print(\"‚úÖ Imports successful\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:agent.storage.message_store:Message store initialized at /tmp/messages.db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Neo4j Status\n",
    "\n",
    "Verify that Neo4j is running before starting the demo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T13:58:28.296777Z",
     "start_time": "2026-01-14T13:58:28.139855Z"
    }
   },
   "source": "",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neo4j is running\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T20:57:34.266206Z",
     "start_time": "2026-01-17T20:57:33.968328Z"
    }
   },
   "source": [
    "# Initialize LLM client\n",
    "llm_client = get_llm_client()\n",
    "print(f\"‚úÖ LLM Client initialized: {llm_client.model_name}\")\n",
    "\n",
    "# Initialize Graphiti client\n",
    "# graphiti_client = await get_graphiti_client()\n",
    "# await graphiti_client.initialize()\n",
    "# print(\"‚úÖ Graphiti Client initialized\", graphiti_client._initialized)\n",
    "\n",
    "# Get agent app\n",
    "agent = get_agent_app()\n",
    "print(\"‚úÖ Agent Graph compiled\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:agent.graph:Initializing global agent application\n",
      "INFO:agent.graph:Creating improved agent graph with conflict resolution chain...\n",
      "INFO:agent.graph:Improved agent graph compiled successfully with conflict resolution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM Client initialized: lapa\n",
      "‚úÖ Agent Graph compiled\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test LLM Connection\n",
    "\n",
    "Let's verify that our LLM is working and responds in Ukrainian"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T21:11:18.079097Z",
     "start_time": "2026-01-17T21:11:16.560988Z"
    }
   },
   "source": [
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"–¢–∏ - –∫–æ—Ä–∏—Å–Ω–∏–π AI –∞—Å–∏—Å—Ç–µ–Ω—Ç.\"},\n",
    "    {\"role\": \"user\", \"content\": \"–ü—Ä–∏–≤—ñ—Ç! –Ø–∫ —Å–ø—Ä–∞–≤–∏?\"}\n",
    "]\n",
    "\n",
    "response = await llm_client.generate_async(test_messages)\n",
    "print(\"LLM Response:\")\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.379681 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.801287 seconds\n",
      "ERROR:clients.llm_client:Error generating response: Connection error.\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mConnectError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:101\u001B[39m, in \u001B[36mmap_httpcore_exceptions\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    100\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m101\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394\u001B[39m, in \u001B[36mAsyncHTTPTransport.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    393\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m--> \u001B[39m\u001B[32m394\u001B[39m     resp = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pool.handle_async_request(req)\n\u001B[32m    396\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256\u001B[39m, in \u001B[36mAsyncConnectionPool.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    255\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._close_connections(closing)\n\u001B[32m--> \u001B[39m\u001B[32m256\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    258\u001B[39m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[32m    259\u001B[39m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236\u001B[39m, in \u001B[36mAsyncConnectionPool.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    234\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    235\u001B[39m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m connection.handle_async_request(\n\u001B[32m    237\u001B[39m         pool_request.request\n\u001B[32m    238\u001B[39m     )\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[32m    240\u001B[39m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[32m    241\u001B[39m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[32m    242\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    243\u001B[39m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:101\u001B[39m, in \u001B[36mAsyncHTTPConnection.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    100\u001B[39m     \u001B[38;5;28mself\u001B[39m._connect_failed = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m101\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[32m    103\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connection.handle_async_request(request)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:78\u001B[39m, in \u001B[36mAsyncHTTPConnection.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m     77\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m78\u001B[39m     stream = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connect(request)\n\u001B[32m     80\u001B[39m     ssl_object = stream.get_extra_info(\u001B[33m\"\u001B[39m\u001B[33mssl_object\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:124\u001B[39m, in \u001B[36mAsyncHTTPConnection._connect\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[33m\"\u001B[39m\u001B[33mconnect_tcp\u001B[39m\u001B[33m\"\u001B[39m, logger, request, kwargs) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m--> \u001B[39m\u001B[32m124\u001B[39m     stream = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._network_backend.connect_tcp(**kwargs)\n\u001B[32m    125\u001B[39m     trace.return_value = stream\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_backends/auto.py:31\u001B[39m, in \u001B[36mAutoBackend.connect_tcp\u001B[39m\u001B[34m(self, host, port, timeout, local_address, socket_options)\u001B[39m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._init_backend()\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backend.connect_tcp(\n\u001B[32m     32\u001B[39m     host,\n\u001B[32m     33\u001B[39m     port,\n\u001B[32m     34\u001B[39m     timeout=timeout,\n\u001B[32m     35\u001B[39m     local_address=local_address,\n\u001B[32m     36\u001B[39m     socket_options=socket_options,\n\u001B[32m     37\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:113\u001B[39m, in \u001B[36mAnyIOBackend.connect_tcp\u001B[39m\u001B[34m(self, host, port, timeout, local_address, socket_options)\u001B[39m\n\u001B[32m    108\u001B[39m exc_map = {\n\u001B[32m    109\u001B[39m     \u001B[38;5;167;01mTimeoutError\u001B[39;00m: ConnectTimeout,\n\u001B[32m    110\u001B[39m     \u001B[38;5;167;01mOSError\u001B[39;00m: ConnectError,\n\u001B[32m    111\u001B[39m     anyio.BrokenResourceError: ConnectError,\n\u001B[32m    112\u001B[39m }\n\u001B[32m--> \u001B[39m\u001B[32m113\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[32m    114\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m anyio.fail_after(timeout):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/contextlib.py:158\u001B[39m, in \u001B[36m_GeneratorContextManager.__exit__\u001B[39m\u001B[34m(self, typ, value, traceback)\u001B[39m\n\u001B[32m    157\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m158\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgen\u001B[49m\u001B[43m.\u001B[49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    160\u001B[39m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[32m    161\u001B[39m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[32m    162\u001B[39m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py:14\u001B[39m, in \u001B[36mmap_exceptions\u001B[39m\u001B[34m(map)\u001B[39m\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(exc, from_exc):\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m to_exc(exc) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mConnectError\u001B[39m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mConnectError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/openai/_base_client.py:1532\u001B[39m, in \u001B[36mAsyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1531\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1532\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._client.send(\n\u001B[32m   1533\u001B[39m         request,\n\u001B[32m   1534\u001B[39m         stream=stream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._should_stream_response_body(request=request),\n\u001B[32m   1535\u001B[39m         **kwargs,\n\u001B[32m   1536\u001B[39m     )\n\u001B[32m   1537\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m httpx.TimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpx/_client.py:1629\u001B[39m, in \u001B[36mAsyncClient.send\u001B[39m\u001B[34m(self, request, stream, auth, follow_redirects)\u001B[39m\n\u001B[32m   1627\u001B[39m auth = \u001B[38;5;28mself\u001B[39m._build_request_auth(request, auth)\n\u001B[32m-> \u001B[39m\u001B[32m1629\u001B[39m response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._send_handling_auth(\n\u001B[32m   1630\u001B[39m     request,\n\u001B[32m   1631\u001B[39m     auth=auth,\n\u001B[32m   1632\u001B[39m     follow_redirects=follow_redirects,\n\u001B[32m   1633\u001B[39m     history=[],\n\u001B[32m   1634\u001B[39m )\n\u001B[32m   1635\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpx/_client.py:1657\u001B[39m, in \u001B[36mAsyncClient._send_handling_auth\u001B[39m\u001B[34m(self, request, auth, follow_redirects, history)\u001B[39m\n\u001B[32m   1656\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1657\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._send_handling_redirects(\n\u001B[32m   1658\u001B[39m         request,\n\u001B[32m   1659\u001B[39m         follow_redirects=follow_redirects,\n\u001B[32m   1660\u001B[39m         history=history,\n\u001B[32m   1661\u001B[39m     )\n\u001B[32m   1662\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpx/_client.py:1694\u001B[39m, in \u001B[36mAsyncClient._send_handling_redirects\u001B[39m\u001B[34m(self, request, follow_redirects, history)\u001B[39m\n\u001B[32m   1692\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m hook(request)\n\u001B[32m-> \u001B[39m\u001B[32m1694\u001B[39m response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._send_single_request(request)\n\u001B[32m   1695\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpx/_client.py:1730\u001B[39m, in \u001B[36mAsyncClient._send_single_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m   1729\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=request):\n\u001B[32m-> \u001B[39m\u001B[32m1730\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m transport.handle_async_request(request)\n\u001B[32m   1732\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, AsyncByteStream)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:393\u001B[39m, in \u001B[36mAsyncHTTPTransport.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    381\u001B[39m req = httpcore.Request(\n\u001B[32m    382\u001B[39m     method=request.method,\n\u001B[32m    383\u001B[39m     url=httpcore.URL(\n\u001B[32m   (...)\u001B[39m\u001B[32m    391\u001B[39m     extensions=request.extensions,\n\u001B[32m    392\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m393\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m    394\u001B[39m     resp = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pool.handle_async_request(req)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/contextlib.py:158\u001B[39m, in \u001B[36m_GeneratorContextManager.__exit__\u001B[39m\u001B[34m(self, typ, value, traceback)\u001B[39m\n\u001B[32m    157\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m158\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgen\u001B[49m\u001B[43m.\u001B[49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    160\u001B[39m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[32m    161\u001B[39m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[32m    162\u001B[39m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:118\u001B[39m, in \u001B[36mmap_httpcore_exceptions\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    117\u001B[39m message = \u001B[38;5;28mstr\u001B[39m(exc)\n\u001B[32m--> \u001B[39m\u001B[32m118\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m mapped_exc(message) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n",
      "\u001B[31mConnectError\u001B[39m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mAPIConnectionError\u001B[39m                        Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      1\u001B[39m test_messages = [\n\u001B[32m      2\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mrole\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33msystem\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m–¢–∏ - –∫–æ—Ä–∏—Å–Ω–∏–π AI –∞—Å–∏—Å—Ç–µ–Ω—Ç.\u001B[39m\u001B[33m\"\u001B[39m},\n\u001B[32m      3\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mrole\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m–ü—Ä–∏–≤—ñ—Ç! –Ø–∫ —Å–ø—Ä–∞–≤–∏?\u001B[39m\u001B[33m\"\u001B[39m}\n\u001B[32m      4\u001B[39m ]\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m response = \u001B[38;5;28;01mawait\u001B[39;00m llm_client.generate_async(test_messages)\n\u001B[32m      7\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mLLM Response:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/scripts/../clients/llm_client.py:108\u001B[39m, in \u001B[36mLLMClient.generate_async\u001B[39m\u001B[34m(self, messages, temperature, max_tokens, response_format, **kwargs)\u001B[39m\n\u001B[32m    105\u001B[39m     llm = llm.with_structured_output(response_format)\n\u001B[32m    107\u001B[39m \u001B[38;5;66;03m# Invoke LLM\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m response = \u001B[38;5;28;01mawait\u001B[39;00m llm.ainvoke(lc_messages)\n\u001B[32m    110\u001B[39m \u001B[38;5;66;03m# If structured output, return directly\u001B[39;00m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m response_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5570\u001B[39m, in \u001B[36mRunnableBindingBase.ainvoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   5563\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   5564\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mainvoke\u001B[39m(\n\u001B[32m   5565\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   5568\u001B[39m     **kwargs: Any | \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   5569\u001B[39m ) -> Output:\n\u001B[32m-> \u001B[39m\u001B[32m5570\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.bound.ainvoke(\n\u001B[32m   5571\u001B[39m         \u001B[38;5;28minput\u001B[39m,\n\u001B[32m   5572\u001B[39m         \u001B[38;5;28mself\u001B[39m._merge_configs(config),\n\u001B[32m   5573\u001B[39m         **{**\u001B[38;5;28mself\u001B[39m.kwargs, **kwargs},\n\u001B[32m   5574\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:425\u001B[39m, in \u001B[36mBaseChatModel.ainvoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    415\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    416\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mainvoke\u001B[39m(\n\u001B[32m    417\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    422\u001B[39m     **kwargs: Any,\n\u001B[32m    423\u001B[39m ) -> AIMessage:\n\u001B[32m    424\u001B[39m     config = ensure_config(config)\n\u001B[32m--> \u001B[39m\u001B[32m425\u001B[39m     llm_result = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.agenerate_prompt(\n\u001B[32m    426\u001B[39m         [\u001B[38;5;28mself\u001B[39m._convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[32m    427\u001B[39m         stop=stop,\n\u001B[32m    428\u001B[39m         callbacks=config.get(\u001B[33m\"\u001B[39m\u001B[33mcallbacks\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    429\u001B[39m         tags=config.get(\u001B[33m\"\u001B[39m\u001B[33mtags\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    430\u001B[39m         metadata=config.get(\u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    431\u001B[39m         run_name=config.get(\u001B[33m\"\u001B[39m\u001B[33mrun_name\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    432\u001B[39m         run_id=config.pop(\u001B[33m\"\u001B[39m\u001B[33mrun_id\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m    433\u001B[39m         **kwargs,\n\u001B[32m    434\u001B[39m     )\n\u001B[32m    435\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[32m    436\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAIMessage\u001B[39m\u001B[33m\"\u001B[39m, cast(\u001B[33m\"\u001B[39m\u001B[33mChatGeneration\u001B[39m\u001B[33m\"\u001B[39m, llm_result.generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m]).message\n\u001B[32m    437\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1132\u001B[39m, in \u001B[36mBaseChatModel.agenerate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m   1123\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   1124\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34magenerate_prompt\u001B[39m(\n\u001B[32m   1125\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1129\u001B[39m     **kwargs: Any,\n\u001B[32m   1130\u001B[39m ) -> LLMResult:\n\u001B[32m   1131\u001B[39m     prompt_messages = [p.to_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m-> \u001B[39m\u001B[32m1132\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.agenerate(\n\u001B[32m   1133\u001B[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001B[32m   1134\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1090\u001B[39m, in \u001B[36mBaseChatModel.agenerate\u001B[39m\u001B[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m   1077\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[32m   1078\u001B[39m         \u001B[38;5;28;01mawait\u001B[39;00m asyncio.gather(\n\u001B[32m   1079\u001B[39m             *[\n\u001B[32m   1080\u001B[39m                 run_manager.on_llm_end(\n\u001B[32m   (...)\u001B[39m\u001B[32m   1088\u001B[39m             ]\n\u001B[32m   1089\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1090\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exceptions[\u001B[32m0\u001B[39m]\n\u001B[32m   1091\u001B[39m flattened_outputs = [\n\u001B[32m   1092\u001B[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n\u001B[32m   1093\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[32m   1094\u001B[39m ]\n\u001B[32m   1095\u001B[39m llm_output = \u001B[38;5;28mself\u001B[39m._combine_llm_outputs([res.llm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1359\u001B[39m, in \u001B[36mBaseChatModel._agenerate_with_cache\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1357\u001B[39m     result = generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[32m   1358\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m inspect.signature(\u001B[38;5;28mself\u001B[39m._agenerate).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1359\u001B[39m     result = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._agenerate(\n\u001B[32m   1360\u001B[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001B[32m   1361\u001B[39m     )\n\u001B[32m   1362\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1363\u001B[39m     result = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1634\u001B[39m, in \u001B[36mBaseChatOpenAI._agenerate\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1632\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m raw_response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(raw_response, \u001B[33m\"\u001B[39m\u001B[33mhttp_response\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m   1633\u001B[39m         e.response = raw_response.http_response  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1634\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m   1635\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   1636\u001B[39m     \u001B[38;5;28mself\u001B[39m.include_response_headers\n\u001B[32m   1637\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m raw_response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1638\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(raw_response, \u001B[33m\"\u001B[39m\u001B[33mheaders\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1639\u001B[39m ):\n\u001B[32m   1640\u001B[39m     generation_info = {\u001B[33m\"\u001B[39m\u001B[33mheaders\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mdict\u001B[39m(raw_response.headers)}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1627\u001B[39m, in \u001B[36mBaseChatOpenAI._agenerate\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1620\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m _construct_lc_result_from_responses_api(\n\u001B[32m   1621\u001B[39m             response,\n\u001B[32m   1622\u001B[39m             schema=original_schema_obj,\n\u001B[32m   1623\u001B[39m             metadata=generation_info,\n\u001B[32m   1624\u001B[39m             output_version=\u001B[38;5;28mself\u001B[39m.output_version,\n\u001B[32m   1625\u001B[39m         )\n\u001B[32m   1626\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1627\u001B[39m         raw_response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.async_client.with_raw_response.create(\n\u001B[32m   1628\u001B[39m             **payload\n\u001B[32m   1629\u001B[39m         )\n\u001B[32m   1630\u001B[39m         response = raw_response.parse()\n\u001B[32m   1631\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/openai/_legacy_response.py:381\u001B[39m, in \u001B[36masync_to_raw_response_wrapper.<locals>.wrapped\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    377\u001B[39m extra_headers[RAW_RESPONSE_HEADER] = \u001B[33m\"\u001B[39m\u001B[33mtrue\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    379\u001B[39m kwargs[\u001B[33m\"\u001B[39m\u001B[33mextra_headers\u001B[39m\u001B[33m\"\u001B[39m] = extra_headers\n\u001B[32m--> \u001B[39m\u001B[32m381\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m cast(LegacyAPIResponse[R], \u001B[38;5;28;01mawait\u001B[39;00m func(*args, **kwargs))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:2678\u001B[39m, in \u001B[36mAsyncCompletions.create\u001B[39m\u001B[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m   2631\u001B[39m \u001B[38;5;129m@required_args\u001B[39m([\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m], [\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m   2632\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate\u001B[39m(\n\u001B[32m   2633\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2675\u001B[39m     timeout: \u001B[38;5;28mfloat\u001B[39m | httpx.Timeout | \u001B[38;5;28;01mNone\u001B[39;00m | NotGiven = not_given,\n\u001B[32m   2676\u001B[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001B[32m   2677\u001B[39m     validate_response_format(response_format)\n\u001B[32m-> \u001B[39m\u001B[32m2678\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._post(\n\u001B[32m   2679\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m/chat/completions\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2680\u001B[39m         body=\u001B[38;5;28;01mawait\u001B[39;00m async_maybe_transform(\n\u001B[32m   2681\u001B[39m             {\n\u001B[32m   2682\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m: messages,\n\u001B[32m   2683\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: model,\n\u001B[32m   2684\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33maudio\u001B[39m\u001B[33m\"\u001B[39m: audio,\n\u001B[32m   2685\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mfrequency_penalty\u001B[39m\u001B[33m\"\u001B[39m: frequency_penalty,\n\u001B[32m   2686\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mfunction_call\u001B[39m\u001B[33m\"\u001B[39m: function_call,\n\u001B[32m   2687\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mfunctions\u001B[39m\u001B[33m\"\u001B[39m: functions,\n\u001B[32m   2688\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mlogit_bias\u001B[39m\u001B[33m\"\u001B[39m: logit_bias,\n\u001B[32m   2689\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mlogprobs\u001B[39m\u001B[33m\"\u001B[39m: logprobs,\n\u001B[32m   2690\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmax_completion_tokens\u001B[39m\u001B[33m\"\u001B[39m: max_completion_tokens,\n\u001B[32m   2691\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmax_tokens\u001B[39m\u001B[33m\"\u001B[39m: max_tokens,\n\u001B[32m   2692\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: metadata,\n\u001B[32m   2693\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmodalities\u001B[39m\u001B[33m\"\u001B[39m: modalities,\n\u001B[32m   2694\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mn\u001B[39m\u001B[33m\"\u001B[39m: n,\n\u001B[32m   2695\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mparallel_tool_calls\u001B[39m\u001B[33m\"\u001B[39m: parallel_tool_calls,\n\u001B[32m   2696\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mprediction\u001B[39m\u001B[33m\"\u001B[39m: prediction,\n\u001B[32m   2697\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mpresence_penalty\u001B[39m\u001B[33m\"\u001B[39m: presence_penalty,\n\u001B[32m   2698\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mprompt_cache_key\u001B[39m\u001B[33m\"\u001B[39m: prompt_cache_key,\n\u001B[32m   2699\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mprompt_cache_retention\u001B[39m\u001B[33m\"\u001B[39m: prompt_cache_retention,\n\u001B[32m   2700\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mreasoning_effort\u001B[39m\u001B[33m\"\u001B[39m: reasoning_effort,\n\u001B[32m   2701\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mresponse_format\u001B[39m\u001B[33m\"\u001B[39m: response_format,\n\u001B[32m   2702\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33msafety_identifier\u001B[39m\u001B[33m\"\u001B[39m: safety_identifier,\n\u001B[32m   2703\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mseed\u001B[39m\u001B[33m\"\u001B[39m: seed,\n\u001B[32m   2704\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mservice_tier\u001B[39m\u001B[33m\"\u001B[39m: service_tier,\n\u001B[32m   2705\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstop\u001B[39m\u001B[33m\"\u001B[39m: stop,\n\u001B[32m   2706\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstore\u001B[39m\u001B[33m\"\u001B[39m: store,\n\u001B[32m   2707\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m: stream,\n\u001B[32m   2708\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstream_options\u001B[39m\u001B[33m\"\u001B[39m: stream_options,\n\u001B[32m   2709\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtemperature\u001B[39m\u001B[33m\"\u001B[39m: temperature,\n\u001B[32m   2710\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtool_choice\u001B[39m\u001B[33m\"\u001B[39m: tool_choice,\n\u001B[32m   2711\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtools\u001B[39m\u001B[33m\"\u001B[39m: tools,\n\u001B[32m   2712\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtop_logprobs\u001B[39m\u001B[33m\"\u001B[39m: top_logprobs,\n\u001B[32m   2713\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtop_p\u001B[39m\u001B[33m\"\u001B[39m: top_p,\n\u001B[32m   2714\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m: user,\n\u001B[32m   2715\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mverbosity\u001B[39m\u001B[33m\"\u001B[39m: verbosity,\n\u001B[32m   2716\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mweb_search_options\u001B[39m\u001B[33m\"\u001B[39m: web_search_options,\n\u001B[32m   2717\u001B[39m             },\n\u001B[32m   2718\u001B[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001B[32m   2719\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m stream\n\u001B[32m   2720\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001B[32m   2721\u001B[39m         ),\n\u001B[32m   2722\u001B[39m         options=make_request_options(\n\u001B[32m   2723\u001B[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001B[32m   2724\u001B[39m         ),\n\u001B[32m   2725\u001B[39m         cast_to=ChatCompletion,\n\u001B[32m   2726\u001B[39m         stream=stream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   2727\u001B[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001B[32m   2728\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/openai/_base_client.py:1797\u001B[39m, in \u001B[36mAsyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1784\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1785\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1792\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_AsyncStreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1793\u001B[39m ) -> ResponseT | _AsyncStreamT:\n\u001B[32m   1794\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1795\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=\u001B[38;5;28;01mawait\u001B[39;00m async_to_httpx_files(files), **options\n\u001B[32m   1796\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1797\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Creator/PythonProject/graphity_lapa/.venv/lib/python3.12/site-packages/openai/_base_client.py:1564\u001B[39m, in \u001B[36mAsyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1561\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m   1563\u001B[39m     log.debug(\u001B[33m\"\u001B[39m\u001B[33mRaising connection error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1564\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m APIConnectionError(request=request) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m   1566\u001B[39m log.debug(\n\u001B[32m   1567\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mHTTP Response: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m'\u001B[39m,\n\u001B[32m   1568\u001B[39m     request.method,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1572\u001B[39m     response.headers,\n\u001B[32m   1573\u001B[39m )\n\u001B[32m   1574\u001B[39m log.debug(\u001B[33m\"\u001B[39m\u001B[33mrequest_id: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m, response.headers.get(\u001B[33m\"\u001B[39m\u001B[33mx-request-id\u001B[39m\u001B[33m\"\u001B[39m))\n",
      "\u001B[31mAPIConnectionError\u001B[39m: Connection error."
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get graph statistics\n",
    "stats = await graphiti_client.get_graph_stats()\n",
    "print(f\"üìä Graph Memory Stats:\")\n",
    "print(f\"   Nodes: {stats['node_count']}\")\n",
    "print(f\"   Relationships: {stats['relationship_count']}\")\n",
    "print(f\"\\nüí° The agent is learning and building a knowledge graph!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get graph statistics\n",
    "stats = await graphiti_client.get_graph_stats()\n",
    "print(f\"üìä Graph Stats:\")\n",
    "print(f\"   Nodes: {stats['node_count']}\")\n",
    "print(f\"   Relationships: {stats['relationship_count']}\")\n",
    "\n",
    "# Search for specific information\n",
    "search_results = await graphiti_client.search(\"–û–ª–µ–∫—Å–∞–Ω–¥—Ä –ö–∏—ó–≤\")\n",
    "print(f\"\\nüîç Search results for '–û–ª–µ–∫—Å–∞–Ω–¥—Ä –ö–∏—ó–≤': {len(search_results)} found\")\n",
    "for i, search_item in enumerate(search_results[:3], 1):\n",
    "    print(f\"   {i}. {search_item.get('content', 'N/A')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Second Conversation: Testing Memory Recall\n",
    "\n",
    "Now let's ask a question that requires recalling information from previous conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Third Conversation: More Complex Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Knowledge Graph\n",
    "\n",
    "Let's query Neo4j directly to see what entities and relationships were created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Demonstrated:\n",
    "1. ‚úÖ **Hosted Lapa LLM** - Ukrainian language model via Lapathon API\n",
    "2. ‚úÖ **Hosted Qwen Embeddings** - Semantic search using hosted embeddings\n",
    "3. ‚úÖ **Graphiti Memory** - Temporal knowledge graph for long-term memory\n",
    "4. ‚úÖ **LangGraph Agent** - Three-node pipeline (retrieve ‚Üí generate ‚Üí save)\n",
    "5. ‚úÖ **Memory Recall** - Context-aware responses using graph memory\n",
    "\n",
    "### Architecture:\n",
    "- **LLM**: Lapa model @ http://146.59.127.106:4000\n",
    "- **Embeddings**: text-embedding-qwen (hosted)\n",
    "- **Memory**: Graphiti + Neo4j graph database\n",
    "- **Agent**: LangGraph with persistent state\n",
    "\n",
    "### Next Steps:\n",
    "1. Explore Neo4j Browser: http://localhost:7474\n",
    "2. Try different conversation topics\n",
    "3. Test memory across multiple sessions\n",
    "4. Experiment with Mamay model (change VLLM_MODEL_NAME=mamay in .env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "### What We Demonstrated:\n",
    "1. ‚úÖ Lapa LLM integration via vLLM with structured outputs\n",
    "2. ‚úÖ Graphiti knowledge graph for long-term memory\n",
    "3. ‚úÖ LangGraph agent orchestration with state management\n",
    "4. ‚úÖ Memory retrieval and contextual responses\n",
    "5. ‚úÖ Graph visualization and querying\n",
    "\n",
    "### Key Features:\n",
    "- **Temporal Memory**: Graphiti tracks when information was learned\n",
    "- **Semantic Search**: Hybrid search (embeddings + BM25 + graph traversal)\n",
    "- **Context Awareness**: Agent uses retrieved memories to personalize responses\n",
    "- **Ukrainian Support**: Lapa LLM optimized for Ukrainian language\n",
    "\n",
    "### Next Steps:\n",
    "1. Add more conversations to build richer memory\n",
    "2. Experiment with different query types\n",
    "3. Visualize graph in Neo4j Browser (http://localhost:7474)\n",
    "4. Test with multiple users/sessions\n",
    "5. Implement memory cleanup strategies for old data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clear all graph data\n",
    "# from neo4j import AsyncGraphDatabase\n",
    "#\n",
    "# async def clear_graph():\n",
    "#     driver = AsyncGraphDatabase.driver(\n",
    "#         settings.neo4j_uri,\n",
    "#         auth=(settings.neo4j_user, settings.neo4j_password)\n",
    "#     )\n",
    "#     async with driver.session(database=settings.neo4j_database) as session:\n",
    "#         await session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "#     await driver.close()\n",
    "#     print(\"‚úÖ Graph cleared\")\n",
    "#\n",
    "# await clear_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
