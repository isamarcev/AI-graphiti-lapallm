{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphiti + LangGraph + Lapa LLM Demo\n",
    "\n",
    "This notebook demonstrates an AI agent with long-term memory using:\n",
    "- **Lapa LLM** - Ukrainian language model via hosted Lapathon API\n",
    "- **Graphiti** - Temporal knowledge graph for memory\n",
    "- **LangGraph** - Agent orchestration\n",
    "- **Neo4j** - Graph database storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Test Graphiti Message Alternation Fix\n",
    "\n",
    "Test that our wrapper correctly handles message role alternation issues"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T13:57:42.327318Z",
     "start_time": "2026-01-14T13:57:27.189997Z"
    }
   },
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from clients.graphiti_client import get_graphiti_client\n",
    "\n",
    "# Enable detailed logging\n",
    "logging.getLogger('clients.graphiti_client_wrapper').setLevel(logging.DEBUG)\n",
    "\n",
    "async def test_graphiti_simple():\n",
    "    \"\"\"Test simple episode to see if alternation works.\"\"\"\n",
    "    print(\"üß™ Testing Graphiti message alternation fix...\")\n",
    "    \n",
    "    # Initialize client\n",
    "    graphiti = await get_graphiti_client()\n",
    "    print(\"‚úì Client initialized\")\n",
    "    \n",
    "    # Test with simple episode\n",
    "    episode_body = \"\"\"User: –ü—Ä–∏–≤—ñ—Ç! –ú–µ–Ω–µ –∑–≤–∞—Ç–∏ –¢–µ—Å—Ç.\n",
    "Assistant: –î—è–∫—É—é –∑–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é. –Ø —ó—ó –∑–±–µ—Ä—ñ–≥.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        episode = await graphiti.add_episode(\n",
    "            episode_body=episode_body,\n",
    "            episode_name=f\"test_notebook_{datetime.now().isoformat()}\",\n",
    "            source_description=\"notebook_test:simple\",\n",
    "            reference_time=datetime.now()\n",
    "        )\n",
    "        print(\"‚úÖ Episode added successfully!\")\n",
    "        \n",
    "        if hasattr(episode, 'nodes') and episode.nodes:\n",
    "            entity_names = [node.name for node in episode.nodes if hasattr(node, 'name')]\n",
    "            print(f\"üì¶ Extracted {len(entity_names)} entities: {entity_names}\")\n",
    "        if hasattr(episode, 'edges') and episode.edges:\n",
    "            print(f\"üîó Extracted {len(episode.edges)} relations\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run test\n",
    "await test_graphiti_simple()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graphiti already initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Graphiti message alternation fix...\n",
      "‚úì Client initialized\n",
      "‚úÖ Episode added successfully!\n",
      "üì¶ Extracted 2 entities: ['User', 'Test']\n",
      "üîó Extracted 1 relations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T13:58:25.581001Z",
     "start_time": "2026-01-14T13:58:22.571313Z"
    }
   },
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import our modules\n",
    "from config.settings import settings\n",
    "from clients.llm_client import get_llm_client\n",
    "from clients.graphiti_client import get_graphiti_client\n",
    "from agent.graph import get_agent_app\n",
    "from agent.state import create_initial_state\n",
    "from utils.langsmith_setup import setup_langsmith\n",
    "\n",
    "# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è LangSmith\n",
    "setup_langsmith()\n",
    "print(\"‚úÖ Imports successful\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangSmith tracing enabled for project: ihor_llm\n",
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Neo4j Status\n",
    "\n",
    "Verify that Neo4j is running before starting the demo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T13:58:28.296777Z",
     "start_time": "2026-01-14T13:58:28.139855Z"
    }
   },
   "source": [
    "async def check_neo4j():\n",
    "    \"\"\"Verify Neo4j connection\"\"\"\n",
    "    try:\n",
    "        from neo4j import AsyncGraphDatabase\n",
    "        driver = AsyncGraphDatabase.driver(\n",
    "            settings.neo4j_uri,\n",
    "            auth=(settings.neo4j_user, settings.neo4j_password)\n",
    "        )\n",
    "        async with driver.session() as session:\n",
    "            await session.run(\"RETURN 1\")\n",
    "        await driver.close()\n",
    "        print(\"‚úÖ Neo4j is running\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Neo4j not accessible: {e}\")\n",
    "        print(\"   Start with: docker-compose up -d\")\n",
    "        return False\n",
    "\n",
    "await check_neo4j()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neo4j is running\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T13:58:33.569172Z",
     "start_time": "2026-01-14T13:58:33.504199Z"
    }
   },
   "source": [
    "# Initialize LLM client\n",
    "llm_client = get_llm_client()\n",
    "print(f\"‚úÖ LLM Client initialized: {llm_client.model_name}\")\n",
    "\n",
    "# Initialize Graphiti client\n",
    "graphiti_client = await get_graphiti_client()\n",
    "# await graphiti_client.initialize()\n",
    "print(\"‚úÖ Graphiti Client initialized\", graphiti_client._initialized)\n",
    "\n",
    "# Get agent app\n",
    "agent = get_agent_app()\n",
    "print(\"‚úÖ Agent Graph compiled\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:clients.graphiti_client:Graphiti already initialized\n",
      "INFO:agent.graph:Initializing global agent application\n",
      "INFO:agent.graph:Creating knowledge-centered agent graph with separated responses...\n",
      "INFO:agent.graph:Knowledge-centered agent graph compiled successfully with separated responses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM Client initialized: lapa\n",
      "‚úÖ Graphiti Client initialized True\n",
      "‚úÖ Agent Graph compiled\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test LLM Connection\n",
    "\n",
    "Let's verify that our LLM is working and responds in Ukrainian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T20:29:37.093618Z",
     "start_time": "2026-01-13T20:29:34.945703Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://146.59.127.106:4000/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:clients.llm_client:Token usage: {'prompt_tokens': 21, 'completion_tokens': 29, 'total_tokens': 50}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: ChatCompletion(id='chatcmpl-9c05c986b7dc6234', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='–ü—Ä–∏–≤—ñ—Ç! –Ø —Ä–∞–¥–∏–π/—Ä–∞–¥–∞ —Ç–µ–±–µ –±–∞—á–∏—Ç–∏. –Ø —Ç—É—Ç, —â–æ–± –¥–æ–ø–æ–º–æ–≥—Ç–∏ —Ç–æ–±—ñ –∑ –±—É–¥—å-—è–∫–∏–º–∏ –ø–∏—Ç–∞–Ω–Ω—è–º–∏. –Ø–∫ —è –º–æ–∂—É —Ç–æ–±—ñ –¥–æ–ø–æ–º–æ–≥—Ç–∏ —Å—å–æ–≥–æ–¥–Ω—ñ?', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None), provider_specific_fields={'stop_reason': 106})], created=1768336175, model='lapa', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=29, prompt_tokens=21, total_tokens=50, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "LLM Response:\n",
      "–ü—Ä–∏–≤—ñ—Ç! –Ø —Ä–∞–¥–∏–π/—Ä–∞–¥–∞ —Ç–µ–±–µ –±–∞—á–∏—Ç–∏. –Ø —Ç—É—Ç, —â–æ–± –¥–æ–ø–æ–º–æ–≥—Ç–∏ —Ç–æ–±—ñ –∑ –±—É–¥—å-—è–∫–∏–º–∏ –ø–∏—Ç–∞–Ω–Ω—è–º–∏. –Ø–∫ —è –º–æ–∂—É —Ç–æ–±—ñ –¥–æ–ø–æ–º–æ–≥—Ç–∏ —Å—å–æ–≥–æ–¥–Ω—ñ?\n"
     ]
    }
   ],
   "source": [
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"–¢–∏ - –∫–æ—Ä–∏—Å–Ω–∏–π AI –∞—Å–∏—Å—Ç–µ–Ω—Ç.\"},\n",
    "    {\"role\": \"user\", \"content\": \"–ü—Ä–∏–≤—ñ—Ç! –Ø–∫ —Å–ø—Ä–∞–≤–∏?\"}\n",
    "]\n",
    "\n",
    "response = await llm_client.generate_async(test_messages)\n",
    "print(\"LLM Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get graph statistics\n",
    "stats = await graphiti_client.get_graph_stats()\n",
    "print(f\"üìä Graph Memory Stats:\")\n",
    "print(f\"   Nodes: {stats['node_count']}\")\n",
    "print(f\"   Relationships: {stats['relationship_count']}\")\n",
    "print(f\"\\nüí° The agent is learning and building a knowledge graph!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get graph statistics\n",
    "stats = await graphiti_client.get_graph_stats()\n",
    "print(f\"üìä Graph Stats:\")\n",
    "print(f\"   Nodes: {stats['node_count']}\")\n",
    "print(f\"   Relationships: {stats['relationship_count']}\")\n",
    "\n",
    "# Search for specific information\n",
    "search_results = await graphiti_client.search(\"–û–ª–µ–∫—Å–∞–Ω–¥—Ä –ö–∏—ó–≤\")\n",
    "print(f\"\\nüîç Search results for '–û–ª–µ–∫—Å–∞–Ω–¥—Ä –ö–∏—ó–≤': {len(search_results)} found\")\n",
    "for i, search_item in enumerate(search_results[:3], 1):\n",
    "    print(f\"   {i}. {search_item.get('content', 'N/A')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Second Conversation: Testing Memory Recall\n",
    "\n",
    "Now let's ask a question that requires recalling information from previous conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Third Conversation: More Complex Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Knowledge Graph\n",
    "\n",
    "Let's query Neo4j directly to see what entities and relationships were created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Demonstrated:\n",
    "1. ‚úÖ **Hosted Lapa LLM** - Ukrainian language model via Lapathon API\n",
    "2. ‚úÖ **Hosted Qwen Embeddings** - Semantic search using hosted embeddings\n",
    "3. ‚úÖ **Graphiti Memory** - Temporal knowledge graph for long-term memory\n",
    "4. ‚úÖ **LangGraph Agent** - Three-node pipeline (retrieve ‚Üí generate ‚Üí save)\n",
    "5. ‚úÖ **Memory Recall** - Context-aware responses using graph memory\n",
    "\n",
    "### Architecture:\n",
    "- **LLM**: Lapa model @ http://146.59.127.106:4000\n",
    "- **Embeddings**: text-embedding-qwen (hosted)\n",
    "- **Memory**: Graphiti + Neo4j graph database\n",
    "- **Agent**: LangGraph with persistent state\n",
    "\n",
    "### Next Steps:\n",
    "1. Explore Neo4j Browser: http://localhost:7474\n",
    "2. Try different conversation topics\n",
    "3. Test memory across multiple sessions\n",
    "4. Experiment with Mamay model (change VLLM_MODEL_NAME=mamay in .env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "### What We Demonstrated:\n",
    "1. ‚úÖ Lapa LLM integration via vLLM with structured outputs\n",
    "2. ‚úÖ Graphiti knowledge graph for long-term memory\n",
    "3. ‚úÖ LangGraph agent orchestration with state management\n",
    "4. ‚úÖ Memory retrieval and contextual responses\n",
    "5. ‚úÖ Graph visualization and querying\n",
    "\n",
    "### Key Features:\n",
    "- **Temporal Memory**: Graphiti tracks when information was learned\n",
    "- **Semantic Search**: Hybrid search (embeddings + BM25 + graph traversal)\n",
    "- **Context Awareness**: Agent uses retrieved memories to personalize responses\n",
    "- **Ukrainian Support**: Lapa LLM optimized for Ukrainian language\n",
    "\n",
    "### Next Steps:\n",
    "1. Add more conversations to build richer memory\n",
    "2. Experiment with different query types\n",
    "3. Visualize graph in Neo4j Browser (http://localhost:7474)\n",
    "4. Test with multiple users/sessions\n",
    "5. Implement memory cleanup strategies for old data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clear all graph data\n",
    "# from neo4j import AsyncGraphDatabase\n",
    "#\n",
    "# async def clear_graph():\n",
    "#     driver = AsyncGraphDatabase.driver(\n",
    "#         settings.neo4j_uri,\n",
    "#         auth=(settings.neo4j_user, settings.neo4j_password)\n",
    "#     )\n",
    "#     async with driver.session(database=settings.neo4j_database) as session:\n",
    "#         await session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "#     await driver.close()\n",
    "#     print(\"‚úÖ Graph cleared\")\n",
    "#\n",
    "# await clear_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
