"""
Simple Context-Based Answer Node.
Takes retrieved context and user query, returns answer based on context only.
No tools, no ReAct loop - just a single LLM call.
"""

import logging
from typing import Any, Dict

from agent.state import AgentState
from clients.llm_client import get_llm_client
from langsmith import traceable
from config.settings import settings

logger = logging.getLogger(__name__)


@traceable(name="context_answer")
async def context_answer_node(state: AgentState) -> Dict[str, Any]:
    """
    Simple node that answers user query based on retrieved context.
    
    Args:
        state: AgentState with retrieved_context and message_text
        
    Returns:
        State update with response
    """
    logger.info("=== Context Answer Node ===")
    
    # Get inputs
    message_text = state.get("message_text", "")
    relevant_context_list = state.get("relevant_context", [])
    plan = state.get("plan", "")
    
    # Format relevant_context list into string
    if not relevant_context_list:
        context_string = "(–∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π)"
    else:
        context_parts = []
        for i, ctx in enumerate(relevant_context_list, 1):
            content = ctx.get("content", "")
            message_id = ctx.get("message_id", "unknown")
            context_parts.append(f"{i}. {content}\n[–¥–∂–µ—Ä–µ–ª–æ: {message_id}]")
        context_string = "\n\n".join(context_parts)
    
    logger.info(f"Formatted {len(relevant_context_list)} context items")

    system_prompt = """–¢–∏ –∞—Å–∏—Å—Ç–µ–Ω—Ç, —è–∫–∏–π –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –¢–Ü–õ–¨–ö–ò –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–¥–∞–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.

üö´ –ó–ê–ë–û–†–û–ù–ï–ù–û –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –±—É–¥—å-—è–∫—ñ –∑–Ω–∞–Ω–Ω—è –ø–æ–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.
‚úÖ –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò —è–∫—â–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è —î –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ.

–ü–†–ê–í–ò–õ–ê:
1. –Ø–∫—â–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å —î –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ ‚Üí –¥–∞–π –≤—ñ–¥–ø–æ–≤—ñ–¥—å
2. –Ø–∫—â–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –ù–ï–ú–ê–Ñ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ ‚Üí —Å–∫–∞–∂–∏ "–ù–µ –º–∞—é —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ —Ü–µ"
3. –û–ë–û–í'–Ø–ó–ö–û–í–û –≤–∫–∞–∑—É–π –¥–∂–µ—Ä–µ–ª–∞ —É —Ñ–æ—Ä–º–∞—Ç—ñ [–¥–∂–µ—Ä–µ–ª–æ: X]
4. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é"""

    user_prompt = f"""–ö–û–ù–¢–ï–ö–°–¢:
{context_string}

–ó–ê–ü–ò–¢–ê–ù–ù–Ø: {message_text}

–û—Ä—ñ—î–Ω—Ç–æ–≤–Ω–∏–π –ø–ª–∞–Ω –≤–∏–∫–æ–Ω–∞–Ω–Ω—è:
{plan}

–í–Ü–î–ü–û–í–Ü–î–¨:"""

    # Call LLM
    llm_client = get_llm_client()
    
    try:
        response = await llm_client.generate_async(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=settings.temperature
        )
        
        logger.info(f"Generated response: {response[:100]}...")
        
    except Exception as e:
        logger.error(f"LLM call failed: {e}")
        response = "–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ"
    
    return {
        "solve_response": response
    }
