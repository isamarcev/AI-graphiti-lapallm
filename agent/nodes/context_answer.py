"""
Simple Context-Based Answer Node.
Takes retrieved context and user query, returns answer based on context only.
No tools, no ReAct loop - just a single LLM call.
"""

import logging
from typing import Any, Dict

from agent.state import AgentState
from clients.llm_client import get_llm_client
from langsmith import traceable

logger = logging.getLogger(__name__)


@traceable(name="context_answer")
async def context_answer_node(state: AgentState) -> Dict[str, Any]:
    """
    Simple node that answers user query based on retrieved context.
    
    Args:
        state: AgentState with retrieved_context and message_text
        
    Returns:
        State update with response
    """
    logger.info("=== Context Answer Node ===")
    
    # Get inputs
    message_text = state.get("message_text", "")
    retrieved_context = state.get("actualized_context", [])

    # Format context
    if retrieved_context:
        context_parts = []
        for i, ctx in enumerate(retrieved_context, 1):
            content = ctx.get("content", "") or ctx.get("fact", "")
            description = ctx.get("description", "")
            examples = ctx.get("examples", "")
            source = ctx.get("source_msg_uid") or ctx.get("messageid") or "unknown"
            
            part = f"{i}. –§–∞–∫—Ç: {content}"
            if description:
                part += f"\n   –û–ø–∏—Å: {description}"
            if examples:
                # examples can be string or list
                if isinstance(examples, list):
                    part += f"\n   –ü—Ä–∏–∫–ª–∞–¥–∏: {', '.join(examples)}"
                else:
                    part += f"\n   –ü—Ä–∏–∫–ª–∞–¥–∏: {examples}"
            part += f"\n   [–¥–∂–µ—Ä–µ–ª–æ: {source}]"
            context_parts.append(part)
        context_text = "\n\n".join(context_parts)
    else:
        context_text = "(–∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π)"
    logger.info("*" * 50)
    logger.info(f"Retrieved context: {context_text}")
    logger.info("*" * 50)
    # Build prompt
    system_prompt = """–¢–∏ –∞—Å–∏—Å—Ç–µ–Ω—Ç, —è–∫–∏–π –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –¢–Ü–õ–¨–ö–ò –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–¥–∞–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.

üö´ –ó–ê–ë–û–†–û–ù–ï–ù–û –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –±—É–¥—å-—è–∫—ñ –∑–Ω–∞–Ω–Ω—è –ø–æ–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.
‚úÖ –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò —è–∫—â–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è —î –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ.

–ü–†–ê–í–ò–õ–ê:
1. –Ø–∫—â–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å —î –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ ‚Üí –¥–∞–π –∫–æ—Ä–æ—Ç–∫—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å (2-4 —Ä–µ—á–µ–Ω–Ω—è)
2. –Ø–∫—â–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –ù–ï–ú–ê–Ñ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ ‚Üí —Å–∫–∞–∂–∏ "–ù–µ –º–∞—é —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ —Ü–µ"
3. –û–ë–û–í'–Ø–ó–ö–û–í–û –≤–∫–∞–∑—É–π –¥–∂–µ—Ä–µ–ª–∞ —É —Ñ–æ—Ä–º–∞—Ç—ñ [–¥–∂–µ—Ä–µ–ª–æ: X]
4. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é"""

    user_prompt = f"""–ö–û–ù–¢–ï–ö–°–¢:
{context_text}

–ó–ê–ü–ò–¢–ê–ù–ù–Ø: {message_text}

–í–Ü–î–ü–û–í–Ü–î–¨:"""

    # Call LLM
    llm_client = get_llm_client()
    
    try:
        response = await llm_client.generate_async(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.001
        )
        
        logger.info(f"Generated response: {response[:100]}...")
        
    except Exception as e:
        logger.error(f"LLM call failed: {e}")
        response = "–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ"
    
    return {
        "solve_response": response
    }
